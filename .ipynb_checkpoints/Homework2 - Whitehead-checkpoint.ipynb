{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Assignment: Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "Build a 2-layer CNN for MNIST digit classfication. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:\n",
    "\n",
    "Image -> convolution (32 5x5 filters) -> nonlinearity (ReLU) ->  (2x2 max pool) -> convolution (64 5x5 filters) -> nonlinearity (ReLU) -> (2x2 max pool) -> fully connected (256 hidden units) -> nonlinearity (ReLU) -> fully connected (10 hidden units) -> softmax\n",
    "\n",
    "Some tips:\n",
    "- The CNN model might take a while to train. Depending on your machine, you might expect this to take up to half an hour. If you see your validation performance start to plateau, you can kill the training.\n",
    "\n",
    "- Since CNNs a more complex than the logistic regression and MLP models you've worked with before, so you may find it helpful to use a more advanced optimizer. You're model will train faster if you use [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) instead of `tf.train.GradientDescentOptimizer`. A learning rate of 1e-4 is a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-21968cdfbb38>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ptrwh\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\ptrwh\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting official/mnist/dataset.py\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ptrwh\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting official/mnist/dataset.py\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ptrwh\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting official/mnist/dataset.py\\t10k-images-idx3-ubyte.gz\n",
      "Extracting official/mnist/dataset.py\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ptrwh\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"official/mnist/dataset.py\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-6c6740d271b5>:50: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--Make sure things are empty and load graph--#\n",
    "tf.reset_default_graph()\n",
    "g = tf.get_default_graph()\n",
    "\n",
    "#--Image and input placeholders--#\n",
    "X = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "x_cnn = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "#--Create the weights and biases--#\n",
    "\n",
    "#Convolution Weights\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev = 0.1))\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev = 0.1))\n",
    "\n",
    "#Convolution Biases\n",
    "b1 = tf.Variable(tf.zeros([32]))\n",
    "b2 = tf.Variable(tf.zeros([64]))\n",
    "\n",
    "#Connected Weights\n",
    "Wfc1 = tf.Variable(tf.truncated_normal([7*7*64,256], stddev = 0.1))\n",
    "Wfc2 = tf.Variable(tf.truncated_normal([256,10], stddev = 0.1))\n",
    "\n",
    "#Connected Biases\n",
    "bfc1 = tf.Variable(tf.truncated_normal([256], stddev = 0.1))\n",
    "bfc2 = tf.Variable(tf.truncated_normal([10], stddev = 0.1))\n",
    "\n",
    "#--Create and pool the convolution layers--#\n",
    "conv1 = tf.nn.max_pool((tf.nn.relu(tf.nn.conv2d(\n",
    "    x_cnn, W1, strides = [1, 1, 1, 1], padding = \"SAME\") + b1)),\n",
    "    ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n",
    "conv2 = tf.nn.max_pool((tf.nn.relu(tf.nn.conv2d(\n",
    "    conv1, W2, strides = [1, 1, 1, 1], padding = \"SAME\") + b2)),\n",
    "    ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n",
    "#--Create the fully connected layers--#\n",
    "\n",
    "#First reshape the output of pooling convolutions\n",
    "flat = tf.reshape(conv2, [-1, 7*7*64])\n",
    "\n",
    "#Create the connected layers\n",
    "latentscores = tf.nn.relu(tf.matmul(flat, Wfc1) + bfc1)\n",
    "scores = tf.matmul(latentscores, Wfc2) + bfc2\n",
    "\n",
    "#--Create Loss function and training steps--#\n",
    "\n",
    "#loss function\n",
    "avg_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=scores, labels = y))\n",
    "\n",
    "#training\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(avg_loss)\n",
    "\n",
    "#initialize\n",
    "init_all = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Create place to save weights--#\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#--Start the session--#\n",
    "sess = tf.Session()\n",
    "sess.run(init_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [04:37<00:00,  1.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./checkpoints/reg_model.ckpt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##-Run the CNN--#\n",
    "for _ in trange(500):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={X: batch_xs, y: batch_ys})\n",
    "\n",
    "#--Save the weights--#\n",
    "saver.save(sess, \"./checkpoints/reg_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9607999920845032\n"
     ]
    }
   ],
   "source": [
    "#--Test trained model--#\n",
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(scores), 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Test accuracy: {0}'.format(sess.run(accuracy, feed_dict={X: mnist.test.images, y: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short answer\n",
    "\n",
    "1\\. How does the CNN compare in accuracy with yesterday's logistic regression and MLP models? How about training time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN has better accuracy than the MLP model from yesterday. Though, my CNN here had 96% accuracy and took ~4.5 minutes to run, but yesterday, my MLP took 12 seconds to run and had ~90% accuracy. I found that if I increased the training iterations for the MLP yesterday to 1000, which put the run time at ~2minutes, then I also got 95% accuracy with that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. How many trainable parameters are there in the CNN you built for this assignment?\n",
    "\n",
    "*Note: By trainable parameters, I mean individual scalars. For example, a weight matrix that is 10x5 has 50.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[Your answer here]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. When would you use a CNN versus a logistic regression model or an MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, I would use these scaling with complexity of the dataset needing to be analyzed; logictic regression, MLP, and then CNN as the dataset was more complex. The CNN, if I remember from class correctly, would be better for images with color layers (i.e. another level of complexity)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
